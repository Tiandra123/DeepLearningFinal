{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This is the modified notebook content that should be copied into itrackerprep.ipynb\n",
        "\n",
        "# ADD THESE IMPORTS AT THE TOP\n",
        "import sys\n",
        "import json\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')  # Use non-interactive backend for headless execution\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# This parameter will be injected by papermill\n",
        "video_path = \"default_video.mp4\"  # This gets overridden when run via papermill\n",
        "\n",
        "# Install dependencies if not already installed\n",
        "# Uncomment if needed\n",
        "# !pip install mediapipe opencv-python matplotlib pandas seaborn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üéûÔ∏è Step 3: Extract frames using ffmpeg (1 frame per second)\n",
        "import ffmpeg\n",
        "import os\n",
        "\n",
        "frame_folder = \"frames_ffmpeg\"\n",
        "os.makedirs(frame_folder, exist_ok=True)\n",
        "\n",
        "(\n",
        "    ffmpeg\n",
        "    .input(video_path)\n",
        "    .output(f'{frame_folder}/frame_%04d.jpg', r=1)  # r = 1 frame per second\n",
        "    .run()\n",
        ")\n",
        "\n",
        "# Count how many frames were extracted\n",
        "extracted_frames = len([f for f in os.listdir(frame_folder) if f.endswith('.jpg')])\n",
        "print(f\"‚úÖ Extracted {extracted_frames} frame(s) into `{frame_folder}/`\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# üëÅÔ∏è Step 4: Run MediaPipe Face Mesh and save previews\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
        "\n",
        "preview_folder = \"landmark_previews\"\n",
        "os.makedirs(preview_folder, exist_ok=True)\n",
        "\n",
        "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1, color=(0, 255, 0))\n",
        "\n",
        "for filename in sorted(os.listdir(frame_folder))[:10]:  # first 10 frames for preview\n",
        "    if not filename.endswith(\".jpg\"):\n",
        "        continue\n",
        "\n",
        "    image_path = os.path.join(frame_folder, filename)\n",
        "    image = cv2.imread(image_path)\n",
        "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = face_mesh.process(rgb)\n",
        "\n",
        "    if results.multi_face_landmarks:\n",
        "        for face_landmarks in results.multi_face_landmarks:\n",
        "            mp_drawing.draw_landmarks(\n",
        "                image=image,\n",
        "                landmark_list=face_landmarks,\n",
        "                connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
        "                landmark_drawing_spec=drawing_spec,\n",
        "                connection_drawing_spec=drawing_spec\n",
        "            )\n",
        "\n",
        "    out_path = os.path.join(preview_folder, f\"preview_{filename}\")\n",
        "    cv2.imwrite(out_path, image)\n",
        "\n",
        "print(\"‚úÖ Saved face mesh previews to `landmark_previews/`\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "import os\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Initialize MediaPipe\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
        "\n",
        "input_folder = \"frames_ffmpeg\"  # where your full frames are\n",
        "output_folder = \"itracker_inputs\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "# Define landmark indices for eyes and general face box\n",
        "LEFT_EYE = [33, 133]      # outer eye corners (left eye)\n",
        "RIGHT_EYE = [362, 263]    # outer eye corners (right eye)\n",
        "FACE_BOX = [10, 152, 234, 454]  # forehead, chin, left cheek, right cheek\n",
        "\n",
        "def get_bbox(landmarks, indices, width, height, padding=10):\n",
        "    x_coords = [landmarks[i].x * width for i in indices]\n",
        "    y_coords = [landmarks[i].y * height for i in indices]\n",
        "    x_min = max(int(min(x_coords)) - padding, 0)\n",
        "    x_max = min(int(max(x_coords)) + padding, width)\n",
        "    y_min = max(int(min(y_coords)) - padding, 0)\n",
        "    y_max = min(int(max(y_coords)) + padding, height)\n",
        "    return x_min, y_min, x_max, y_max\n",
        "\n",
        "# Process each frame\n",
        "for filename in sorted(os.listdir(input_folder)):\n",
        "    if not filename.endswith(\".jpg\"):\n",
        "        continue\n",
        "\n",
        "    path = os.path.join(input_folder, filename)\n",
        "    image = cv2.imread(path)\n",
        "    h, w, _ = image.shape\n",
        "    rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = face_mesh.process(rgb)\n",
        "\n",
        "    if results.multi_face_landmarks:\n",
        "        face = results.multi_face_landmarks[0]\n",
        "        landmarks = face.landmark\n",
        "\n",
        "        lx1, ly1, lx2, ly2 = get_bbox(landmarks, LEFT_EYE, w, h)\n",
        "        rx1, ry1, rx2, ry2 = get_bbox(landmarks, RIGHT_EYE, w, h)\n",
        "        fx1, fy1, fx2, fy2 = get_bbox(landmarks, FACE_BOX, w, h)\n",
        "\n",
        "        left_eye_crop = image[ly1:ly2, lx1:lx2]\n",
        "        right_eye_crop = image[ry1:ry2, rx1:rx2]\n",
        "        face_crop = image[fy1:fy2, fx1:fx2]\n",
        "\n",
        "        base = filename.replace(\".jpg\", \"\")\n",
        "        cv2.imwrite(f\"{output_folder}/{base}_left_eye.jpg\", left_eye_crop)\n",
        "        cv2.imwrite(f\"{output_folder}/{base}_right_eye.jpg\", right_eye_crop)\n",
        "        cv2.imwrite(f\"{output_folder}/{base}_face.jpg\", face_crop)\n",
        "\n",
        "print(\"‚úÖ Done cropping all eyes and faces.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "output_folder = \"itracker_inputs\"\n",
        "\n",
        "# Get a list of all cropped files\n",
        "cropped_files = sorted(os.listdir(output_folder))\n",
        "\n",
        "# Automatically find a frame that has all three images\n",
        "samples = sorted(set(f.split(\"_\")[0] + \"_\" + f.split(\"_\")[1] for f in cropped_files if \"left_eye\" in f))\n",
        "sample = samples[0]  # pick the first one\n",
        "print(f\"‚úÖ Previewing: {sample}\")\n",
        "\n",
        "# Load images\n",
        "left = Image.open(f\"{output_folder}/{sample}_left_eye.jpg\")\n",
        "right = Image.open(f\"{output_folder}/{sample}_right_eye.jpg\")\n",
        "face = Image.open(f\"{output_folder}/{sample}_face.jpg\")\n",
        "\n",
        "# Plot\n",
        "fig, axs = plt.subplots(1, 3, figsize=(10, 4))\n",
        "axs[0].imshow(left)\n",
        "axs[0].set_title(\"Left Eye\")\n",
        "axs[1].imshow(right)\n",
        "axs[1].set_title(\"Right Eye\")\n",
        "axs[2].imshow(face)\n",
        "axs[2].set_title(\"Face\")\n",
        "\n",
        "for ax in axs:\n",
        "    ax.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "input_folder = \"/content/itracker_inputs\"\n",
        "grid_size = 25\n",
        "\n",
        "# Function to calculate face grid\n",
        "def create_face_grid(img_width, img_height, bbox, grid_size=25):\n",
        "    x1, y1, x2, y2 = bbox\n",
        "    center_x = (x1 + x2) / 2\n",
        "    center_y = (y1 + y2) / 2\n",
        "\n",
        "    norm_x = center_x / img_width\n",
        "    norm_y = center_y / img_height\n",
        "\n",
        "    grid_x = min(int(norm_x * grid_size), grid_size - 1)\n",
        "    grid_y = min(int(norm_y * grid_size), grid_size - 1)\n",
        "\n",
        "    grid = np.zeros((grid_size, grid_size), dtype=np.uint8)\n",
        "    grid[grid_y, grid_x] = 1\n",
        "    return grid\n",
        "\n",
        "# Loop through all face crops and create face grids\n",
        "face_files = sorted([f for f in os.listdir(input_folder) if f.endswith(\"_face.jpg\")])\n",
        "print(f\"üîÅ Creating face grids for {len(face_files)} frames...\")\n",
        "\n",
        "for face_file in face_files:\n",
        "    frame_id = face_file.replace(\"_face.jpg\", \"\")\n",
        "    face_path = os.path.join(input_folder, face_file)\n",
        "\n",
        "    image = cv2.imread(face_path)\n",
        "    h, w, _ = image.shape\n",
        "\n",
        "    # Use entire face image as bbox (simplified assumption)\n",
        "    grid = create_face_grid(w, h, (0, 0, w, h))  # assumes face takes full crop\n",
        "\n",
        "    # Save grid\n",
        "    grid_path = os.path.join(input_folder, f\"{frame_id}_face_grid.npy\")\n",
        "    np.save(grid_path, grid)\n",
        "\n",
        "print(\"‚úÖ Done! Face grids saved as .npy files.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_files = [f for f in os.listdir(input_folder) if f.endswith(\"_face_grid.npy\")]\n",
        "print(f\"‚úÖ Found {len(grid_files)} face grid files:\")\n",
        "print(grid_files[:5])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_path = \"checkpoint (1).pth\"  # or update with the correct filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import time, math\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import numpy as np\n",
        "import torch.utils.model_zoo as model_zoo\n",
        "from torch.autograd.variable import Variable\n",
        "\n",
        "'''\n",
        "Pytorch model for the iTracker.\n",
        "\n",
        "Author: Petr Kellnhofer ( pkel_lnho (at) gmai_l.com // remove underscores and spaces), 2018.\n",
        "\n",
        "Website: http://gazecapture.csail.mit.edu/\n",
        "\n",
        "Cite:\n",
        "\n",
        "Eye Tracking for Everyone\n",
        "K.Krafka*, A. Khosla*, P. Kellnhofer, H. Kannan, S. Bhandarkar, W. Matusik and A. Torralba\n",
        "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016\n",
        "\n",
        "@inproceedings{cvpr2016_gazecapture,\n",
        "Author = {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba},\n",
        "Title = {Eye Tracking for Everyone},\n",
        "Year = {2016},\n",
        "Booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}\n",
        "}\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "class ItrackerImageModel(nn.Module):\n",
        "    # Used for both eyes (with shared weights) and the face (with unqiue weights)\n",
        "    def __init__(self):\n",
        "        super(ItrackerImageModel, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 96, kernel_size=11, stride=4, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.CrossMapLRN2d(size=5, alpha=0.0001, beta=0.75, k=1.0),\n",
        "            nn.Conv2d(96, 256, kernel_size=5, stride=1, padding=2, groups=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            nn.CrossMapLRN2d(size=5, alpha=0.0001, beta=0.75, k=1.0),\n",
        "            nn.Conv2d(256, 384, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(384, 64, kernel_size=1, stride=1, padding=0),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "class FaceImageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FaceImageModel, self).__init__()\n",
        "        self.conv = ItrackerImageModel()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(12*12*64, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class FaceGridModel(nn.Module):\n",
        "    # Model for the face grid pathway\n",
        "    def __init__(self, gridSize = 25):\n",
        "        super(FaceGridModel, self).__init__()\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(gridSize * gridSize, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class ITrackerModel(nn.Module):\n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ITrackerModel, self).__init__()\n",
        "        self.eyeModel = ItrackerImageModel()\n",
        "        self.faceModel = FaceImageModel()\n",
        "        self.gridModel = FaceGridModel()\n",
        "        # Joining both eyes\n",
        "        self.eyesFC = nn.Sequential(\n",
        "            nn.Linear(2*12*12*64, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            )\n",
        "        # Joining everything\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(128+64+128, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 2),\n",
        "            )\n",
        "\n",
        "    def forward(self, faces, eyesLeft, eyesRight, faceGrids):\n",
        "        # Eye nets\n",
        "        xEyeL = self.eyeModel(eyesLeft)\n",
        "        xEyeR = self.eyeModel(eyesRight)\n",
        "        # Cat and FC\n",
        "        xEyes = torch.cat((xEyeL, xEyeR), 1)\n",
        "        xEyes = self.eyesFC(xEyes)\n",
        "\n",
        "        # Face net\n",
        "        xFace = self.faceModel(faces)\n",
        "        xGrid = self.gridModel(faceGrids)\n",
        "\n",
        "        # Cat all\n",
        "        x = torch.cat((xEyes, xFace, xGrid), 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint = torch.load(\"checkpoint (1).pth\", map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "model.eval()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(\"‚úÖ Model loaded successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "input_folder = \"/content/itracker_inputs\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Preprocessing transforms\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "def load_image_tensor(path):\n",
        "    img = Image.open(path).convert(\"RGB\")\n",
        "    return image_transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "def load_face_grid(path):\n",
        "    grid = np.load(path).flatten()\n",
        "    return torch.tensor(grid, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "def load_inputs(frame_id):\n",
        "    base = os.path.join(input_folder, frame_id)\n",
        "    left_eye = load_image_tensor(f\"{base}_left_eye.jpg\")\n",
        "    right_eye = load_image_tensor(f\"{base}_right_eye.jpg\")\n",
        "    face = load_image_tensor(f\"{base}_face.jpg\")\n",
        "    grid = load_face_grid(f\"{base}_face_grid.npy\")\n",
        "    return face, left_eye, right_eye, grid\n",
        "\n",
        "# Find all frames\n",
        "frame_ids = sorted(set(f.replace(\"_left_eye.jpg\", \"\") for f in os.listdir(input_folder) if f.endswith(\"_left_eye.jpg\")))\n",
        "print(f\"üîé Found {len(frame_ids)} frames to predict.\")\n",
        "\n",
        "# Predict\n",
        "results = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for frame_id in frame_ids:\n",
        "        try:\n",
        "            face, left, right, grid = load_inputs(frame_id)\n",
        "            output = model(face, left, right, grid)\n",
        "            x, y = output[0].tolist()\n",
        "            results.append({\"frame_id\": frame_id, \"gaze_x\": x, \"gaze_y\": y})\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed on {frame_id}: {e}\")\n",
        "\n",
        "# Save to CSV\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"gaze_predictions.csv\", index=False)\n",
        "print(\"‚úÖ Gaze predictions saved to 'gaze_predictions.csv'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load predictions\n",
        "df = pd.read_csv(\"gaze_predictions.csv\")\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(df)} gaze predictions.\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract gaze coordinates\n",
        "x = df[\"gaze_x\"]\n",
        "y = df[\"gaze_y\"]\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.kdeplot(\n",
        "    x=x,\n",
        "    y=y,\n",
        "    fill=True,\n",
        "    cmap=\"mako\",\n",
        "    bw_adjust=0.1,\n",
        "    thresh=0.05\n",
        ")\n",
        "plt.title(\"Gaze Heatmap\")\n",
        "plt.xlabel(\"Horizontal Gaze (0 = left, 1 = right)\")\n",
        "plt.ylabel(\"Vertical Gaze (0 = top, 1 = bottom)\")\n",
        "plt.gca().invert_yaxis()  # (0,0) is top-left, like screen coordinates\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def is_eye_contact(x, y, center=0.5, threshold=0.2):\n",
        "    return abs(x - center) < threshold and abs(y - center) < threshold\n",
        "\n",
        "# Add a new column to the DataFrame\n",
        "df[\"eye_contact\"] = df.apply(lambda row: is_eye_contact(row[\"gaze_x\"], row[\"gaze_y\"]), axis=1)\n",
        "\n",
        "# Compute % of frames with eye contact\n",
        "eye_contact_percent = df[\"eye_contact\"].mean() * 100\n",
        "print(f\"‚úÖ Eye contact maintained for {eye_contact_percent:.1f}% of the interview.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if eye_contact_percent > 80:\n",
        "    feedback = f\"‚úÖ Excellent! You maintained strong eye contact for {eye_contact_percent:.1f}% of your interview.\"\n",
        "elif eye_contact_percent > 60:\n",
        "    feedback = f\"üôÇ Great effort! You maintained eye contact for {eye_contact_percent:.1f}%, which is good ‚Äî but there's still room to improve.\"\n",
        "elif eye_contact_percent > 40:\n",
        "    feedback = f\"‚ö†Ô∏è You made eye contact for {eye_contact_percent:.1f}% of the time. Try to stay focused on the camera more often.\"\n",
        "else:\n",
        "    feedback = f\"üëÄ Eye contact was low ({eye_contact_percent:.1f}%). Practicing steady focus on the camera will help you connect better with your interviewer.\"\n",
        "\n",
        "print(feedback)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
